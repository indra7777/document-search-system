{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Model Setup for Document Search System\n",
    "\n",
    "This notebook will help you download and set up GGUF models for the document search system using Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cublas-cu12 (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/numpy-1.26.3.dist-info/METADATA'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Login to Hugging Face (Optional but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.7.14)\n",
      "Using cached huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.33.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0baabb56a34113aa87292e82c8fbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Uncomment and run this if you want to login to Hugging Face\n",
    "# This is optional but recommended for better download speeds\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Models Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Models directory created: /workspace/models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "print(f\"âœ… Models directory created: {models_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Choose and Download a Model\n",
    "\n",
    "Select one of the following options based on your needs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Phi-3 Mini (3.8B) - Recommended for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Downloading Phi-3 Mini GGUF model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0b30a549b64cadbe8e2bb93110b415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phi-3-mini-4k-instruct-q4.gguf:   0%|          | 0.00/2.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Phi-3 Mini model ready at: models/llama-model.gguf\n",
      "ðŸ“Š Model size: 2.23 GB\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import shutil\n",
    "\n",
    "print(\"ðŸ“¥ Downloading Phi-3 Mini GGUF model...\")\n",
    "\n",
    "# Download Phi-3 Mini Q4 GGUF model\n",
    "downloaded_file = hf_hub_download(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    filename=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    local_dir=\"models\",\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "# Move to expected location\n",
    "target_file = models_dir / \"llama-model.gguf\"\n",
    "if target_file.exists():\n",
    "    target_file.unlink()  # Remove existing file\n",
    "\n",
    "shutil.move(downloaded_file, target_file)\n",
    "print(f\"âœ… Phi-3 Mini model ready at: {target_file}\")\n",
    "print(f\"ðŸ“Š Model size: {target_file.stat().st_size / 1024 / 1024 / 1024:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Llama 3.1 8B - Better quality, larger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Downloading better model with wget...\n",
      "\n",
      "ðŸ”„ Trying to download Phi-3 Medium 14B...\n",
      "âŒ Failed to download Phi-3 Medium 14B\n",
      "\n",
      "ðŸ”„ Trying to download Llama 3.2 3B...\n",
      "âœ… Llama 3.2 3B downloaded successfully!\n",
      "ðŸ“Š Model size: 1.88 GB\n",
      "ðŸ“ Location: models/llama-model.gguf\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Direct download with wget\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ðŸ“¥ Downloading better model with wget...\")\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Option 1: Try Phi-3 Medium directly\n",
    "model_urls = [\n",
    "    (\"Phi-3 Medium 14B\", \"https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-gguf/resolve/main/Phi-3-medium-4k-instruct-q4.gguf\"),\n",
    "    (\"Llama 3.2 3B\", \"https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF/resolve/main/llama-3.2-3b-instruct-q4_k_m.gguf\"),\n",
    "    (\"Gemma 2 2B\", \"https://huggingface.co/google/gemma-2-2b-it-GGUF/resolve/main/2b_it_q4_k_m.gguf\")\n",
    "]\n",
    "\n",
    "for model_name, url in model_urls:\n",
    "    print(f\"\\nðŸ”„ Trying to download {model_name}...\")\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            \"wget\", \"-O\", \"models/temp-model.gguf\", url\n",
    "        ], capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            # Success! Move to final location\n",
    "            target_file = models_dir / \"llama-model.gguf\"\n",
    "            if target_file.exists():\n",
    "                target_file.unlink()\n",
    "            \n",
    "            os.rename(\"models/temp-model.gguf\", target_file)\n",
    "            file_size_gb = target_file.stat().st_size / 1024 / 1024 / 1024\n",
    "            print(f\"âœ… {model_name} downloaded successfully!\")\n",
    "            print(f\"ðŸ“Š Model size: {file_size_gb:.2f} GB\")\n",
    "            print(f\"ðŸ“ Location: {target_file}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"âŒ Failed to download {model_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error downloading {model_name}: {e}\")\n",
    "        continue\n",
    "else:\n",
    "    print(\"\\nâš ï¸ All downloads failed. You may need to manually download a GGUF model.\")\n",
    "    print(\"Try visiting https://huggingface.co/models?library=gguf and download any instruct model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Llama 3.2 1B - Fastest, smallest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import shutil\n",
    "\n",
    "print(\"ðŸ“¥ Downloading Llama 3.2 1B GGUF model...\")\n",
    "\n",
    "# Download Llama 3.2 1B Q4_K_M GGUF model\n",
    "downloaded_file = hf_hub_download(\n",
    "    repo_id=\"bartowski/Llama-3.2-1B-Instruct-GGUF\",\n",
    "    filename=\"Llama-3.2-1B-Instruct-Q4_K_M.gguf\",\n",
    "    local_dir=\"models\",\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "# Move to expected location\n",
    "target_file = models_dir / \"llama-model.gguf\"\n",
    "if target_file.exists():\n",
    "    target_file.unlink()  # Remove existing file\n",
    "\n",
    "shutil.move(downloaded_file, target_file)\n",
    "print(f\"âœ… Llama 3.2 1B model ready at: {target_file}\")\n",
    "print(f\"ðŸ“Š Model size: {target_file.stat().st_size / 1024 / 1024 / 1024:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Model Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model successfully installed!\n",
      "ðŸ“ Location: /workspace/models/llama-model.gguf\n",
      "ðŸ“Š Size: 2.23 GB\n",
      "\n",
      "ðŸ§ª Testing model loading...\n",
      "âš ï¸  Model file exists but couldn't test loading: No module named 'llama_cpp'\n",
      "This might be normal if dependencies aren't fully installed yet.\n"
     ]
    }
   ],
   "source": [
    "# Check if model file exists and get info\n",
    "model_path = models_dir / \"llama-model.gguf\"\n",
    "\n",
    "if model_path.exists():\n",
    "    file_size_gb = model_path.stat().st_size / 1024 / 1024 / 1024\n",
    "    print(f\"âœ… Model successfully installed!\")\n",
    "    print(f\"ðŸ“ Location: {model_path.absolute()}\")\n",
    "    print(f\"ðŸ“Š Size: {file_size_gb:.2f} GB\")\n",
    "    \n",
    "    # Try to load the model to verify it works\n",
    "    try:\n",
    "        print(\"\\nðŸ§ª Testing model loading...\")\n",
    "        \n",
    "        # Import our LLM service\n",
    "        import sys\n",
    "        sys.path.append('.')\n",
    "        \n",
    "        from config import Config\n",
    "        from llm_service_cpp import LLMServiceCPP\n",
    "        \n",
    "        config = Config()\n",
    "        llm_service = LLMServiceCPP(config)\n",
    "        \n",
    "        if llm_service.check_model_availability():\n",
    "            print(\"âœ… Model loaded successfully and is working!\")\n",
    "            \n",
    "            # Test generation\n",
    "            test_response = llm_service.llm(\n",
    "                \"<|system|>You are a helpful assistant.<|user|>Say hello!<|assistant|>\",\n",
    "                max_tokens=50,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            print(f\"ðŸ¤– Test response: {test_response['choices'][0]['text'].strip()}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ Model loaded but not responding correctly\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Model file exists but couldn't test loading: {e}\")\n",
    "        print(\"This might be normal if dependencies aren't fully installed yet.\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ Model file not found. Please run one of the download options above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Install System Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing system requirements...\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.53.3)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: docling>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.42.2)\n",
      "Requirement already satisfied: faiss-cpu>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.11.0.post1)\n",
      "Collecting streamlit>=1.28.0 (from -r requirements.txt (line 6))\n",
      "  Using cached streamlit-1.47.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langchain>=0.1.0 (from -r requirements.txt (line 7))\n",
      "  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting chromadb>=0.4.0 (from -r requirements.txt (line 8))\n",
      "  Using cached chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting llama-cpp-python>=0.2.0 (from -r requirements.txt (line 9))\n",
      "  Using cached llama_cpp_python-0.3.14-cp310-cp310-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.2.6)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.3.1)\n",
      "Requirement already satisfied: pypdf2>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (3.0.1)\n",
      "Requirement already satisfied: python-docx>=0.8.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.1.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.32.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch>=2.0.0->-r requirements.txt (line 1)) (69.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.33.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.30.0->-r requirements.txt (line 2)) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->-r requirements.txt (line 3)) (1.7.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->-r requirements.txt (line 3)) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->-r requirements.txt (line 3)) (10.2.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (2.11.7)\n",
      "Requirement already satisfied: docling-core<3.0.0,>=2.42.0 in /usr/local/lib/python3.10/dist-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (2.43.1)\n",
      "Requirement already satisfied: docling-parse<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (4.1.0)\n",
      "Requirement already satisfied: docling-ibm-models<4,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (3.9.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: pypdfium2<5.0.0,>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (4.30.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (2.10.1)\n",
      "Requirement already satisfied: easyocr<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (1.7.2)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (2025.7.14)\n",
      "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: typer<0.17.0,>=0.12.5 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (1.0.2)\n",
      "Requirement already satisfied: marko<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (2.1.4)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: pylatexenc<3.0,>=2.10 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from docling>=1.0.0->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 11)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.0->-r requirements.txt (line 14)) (2.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.12.0->-r requirements.txt (line 15)) (2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->-r requirements.txt (line 16)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->-r requirements.txt (line 16)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->-r requirements.txt (line 16)) (2.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<2,>=1.0.0->docling>=1.0.0->-r requirements.txt (line 4)) (5.9.8)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.10/dist-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (4.21.1)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in /usr/local/lib/python3.10/dist-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (3.78.0)\n",
      "Requirement already satisfied: semchunk<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.10/dist-packages (from docling-ibm-models<4,>=3.6.0->docling>=1.0.0->-r requirements.txt (line 4)) (0.22.1)\n",
      "Requirement already satisfied: jsonlines<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from docling-ibm-models<4,>=3.6.0->docling>=1.0.0->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /usr/local/lib/python3.10/dist-packages (from docling-ibm-models<4,>=3.6.0->docling>=1.0.0->-r requirements.txt (line 4)) (4.12.0.88)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr<2.0,>=1.7->docling>=1.0.0->-r requirements.txt (line 4)) (0.25.2)\n",
      "Requirement already satisfied: python-bidi in /usr/local/lib/python3.10/dist-packages (from easyocr<2.0,>=1.7->docling>=1.0.0->-r requirements.txt (line 4)) (0.6.6)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr<2.0,>=1.7->docling>=1.0.0->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: pyclipper in /usr/local/lib/python3.10/dist-packages (from easyocr<2.0,>=1.7->docling>=1.0.0->-r requirements.txt (line 4)) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from easyocr<2.0,>=1.7->docling>=1.0.0->-r requirements.txt (line 4)) (1.11.1.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4,>=3.6.0->docling>=1.0.0->-r requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (0.17.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->docling>=1.0.0->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->docling>=1.0.0->-r requirements.txt (line 4)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->docling>=1.0.0->-r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.3.0->docling>=1.0.0->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx<2.0.0,>=1.0.2->docling>=1.0.0->-r requirements.txt (line 4)) (3.2.5)\n",
      "Requirement already satisfied: mpire[dill] in /usr/local/lib/python3.10/dist-packages (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (2.10.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.17.0,>=0.12.5->docling>=1.0.0->-r requirements.txt (line 4)) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.17.0,>=0.12.5->docling>=1.0.0->-r requirements.txt (line 4)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.17.0,>=0.12.5->docling>=1.0.0->-r requirements.txt (line 4)) (14.0.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit>=1.28.0->-r requirements.txt (line 6))\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit>=1.28.0->-r requirements.txt (line 6))\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.28.0->-r requirements.txt (line 6)) (5.5.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.28.0->-r requirements.txt (line 6)) (6.31.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.28.0->-r requirements.txt (line 6)) (21.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.28.0->-r requirements.txt (line 6)) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.28.0->-r requirements.txt (line 6)) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.28.0->-r requirements.txt (line 6)) (6.0.0)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit>=1.28.0->-r requirements.txt (line 6))\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=1.28.0->-r requirements.txt (line 6))\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.28.0->-r requirements.txt (line 6)) (6.4)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=1.28.0->-r requirements.txt (line 6)) (1.48.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.28.0->-r requirements.txt (line 6)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.28.0->-r requirements.txt (line 6)) (5.0.2)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain>=0.1.0->-r requirements.txt (line 7))\n",
      "  Using cached langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain>=0.1.0->-r requirements.txt (line 7))\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain>=0.1.0->-r requirements.txt (line 7))\n",
      "  Using cached langsmith-0.4.8-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.1.0->-r requirements.txt (line 7)) (2.0.41)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain>=0.1.0->-r requirements.txt (line 7))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.1.0->-r requirements.txt (line 7)) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain>=0.1.0->-r requirements.txt (line 7)) (2.4)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.0->-r requirements.txt (line 7)) (3.2.3)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.0->-r requirements.txt (line 8)) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.0->-r requirements.txt (line 8)) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 8)) (0.35.0)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.0->-r requirements.txt (line 8)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.0->-r requirements.txt (line 8)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.0->-r requirements.txt (line 8)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.0->-r requirements.txt (line 8)) (1.73.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.0->-r requirements.txt (line 8)) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.0->-r requirements.txt (line 8)) (3.11.0)\n",
      "Collecting httpx>=0.27.0 (from chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/lib/python3/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python>=0.2.0->-r requirements.txt (line 9)) (5.6.3)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb>=0.4.0->-r requirements.txt (line 8)) (2.0.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (2.1.5)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.7.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (0.6.1)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 7))\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 7)) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 8)) (25.2.10)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.35.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling>=1.0.0->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling>=1.0.0->-r requirements.txt (line 4)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling>=1.0.0->-r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 8)) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 8)) (0.21.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 8))\n",
      "  Using cached watchfiles-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 8)) (10.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (0.70.18)\n",
      "Requirement already satisfied: dill>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from multiprocess->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling>=1.0.0->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=1.0.0->-r requirements.txt (line 4)) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=1.0.0->-r requirements.txt (line 4)) (2025.5.10)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=1.0.0->-r requirements.txt (line 4)) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->-r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->-r requirements.txt (line 3)) (3.6.0)\n",
      "Using cached streamlit-1.47.0-py3-none-any.whl (9.9 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached langchain_core-0.3.72-py3-none-any.whl (442 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached langsmith-0.4.8-py3-none-any.whl (367 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "Using cached opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "Using cached watchfiles-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: blinker, bcrypt, backoff, async-timeout, watchfiles, requests-toolbelt, requests-oauthlib, pydeck, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, llama-cpp-python, httpx, google-auth, gitpython, opentelemetry-semantic-conventions, langsmith, kubernetes, opentelemetry-sdk, langchain-core, altair, streamlit, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain, chromadb\n",
      "\u001b[2K  Attempting uninstall: blinker\n",
      "\u001b[2K    Found existing installation: blinker 1.4\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-distutils-installed-package\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m Cannot uninstall blinker 1.4\n",
      "\u001b[31mâ•°â”€>\u001b[0m It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/27\u001b[0m [blinker]\n",
      "\u001b[1A\u001b[2K\n",
      "âœ… All dependencies installed!\n",
      "\n",
      "ðŸŽ‰ Setup Complete!\n",
      "\n",
      "Next steps:\n",
      "1. Build index: python run_search.py --build /path/to/your/documents\n",
      "2. Start web UI: python run_search.py --web\n",
      "3. Or search directly: python run_search.py --search 'your question'\n"
     ]
    }
   ],
   "source": [
    "# Install the main system requirements\n",
    "print(\"ðŸ“¦ Installing system requirements...\")\n",
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "print(\"\\nâœ… All dependencies installed!\")\n",
    "print(\"\\nðŸŽ‰ Setup Complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Build index: python run_search.py --build /path/to/your/documents\")\n",
    "print(\"2. Start web UI: python run_search.py --web\")\n",
    "print(\"3. Or search directly: python run_search.py --search 'your question'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Test the Complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.53.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.7.1)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.33.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch>=1.11.0->sentence_transformers) (69.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Using cached sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "Installing collected packages: scikit-learn, sentence_transformers\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [sentence_transformers]ence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.7.1 sentence_transformers-5.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing complete system...\n",
      "âŒ System test failed: No module named 'llama_cpp'\n",
      "Please check the error and install missing dependencies\n"
     ]
    }
   ],
   "source": [
    "# Quick system test\n",
    "try:\n",
    "    print(\"ðŸ§ª Testing complete system...\")\n",
    "    \n",
    "    # Test imports\n",
    "    from config import Config\n",
    "    from document_processor import DocumentProcessor\n",
    "    from embedding_service import EmbeddingService\n",
    "    from vector_database import VectorDatabase\n",
    "    from llm_service_cpp import LLMServiceCPP\n",
    "    \n",
    "    print(\"âœ… All modules imported successfully\")\n",
    "    \n",
    "    # Test CUDA availability\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"âš ï¸  CUDA not available - will use CPU mode\")\n",
    "    \n",
    "    # Test model loading\n",
    "    config = Config()\n",
    "    config.create_directories()\n",
    "    \n",
    "    print(\"âœ… Configuration loaded\")\n",
    "    print(\"âœ… System ready for document processing!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ System test failed: {e}\")\n",
    "    print(\"Please check the error and install missing dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing llama-cpp-python with CUDA support...\n",
      "Using RTX 4090 for accelerated compilation...\n",
      "Using pip 25.1.1 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m217.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
      "  Using pip 25.1.1 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
      "  Collecting scikit-build-core>=0.9.2 (from scikit-build-core[pyproject]>=0.9.2)\n",
      "    Obtaining dependency information for scikit-build-core>=0.9.2 from https://files.pythonhosted.org/packages/45/23/0ffa0df7550ca0535f6e03b9a9ab2bf0495ac62e15fd322544c98321a10c/scikit_build_core-0.11.5-py3-none-any.whl.metadata\n",
      "    Using cached scikit_build_core-0.11.5-py3-none-any.whl.metadata (18 kB)\n",
      "  Collecting exceptiongroup>=1.0 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Obtaining dependency information for exceptiongroup>=1.0 from https://files.pythonhosted.org/packages/36/f4/c6e662dade71f56cd2f3735141b265c3c79293c109549c1e6933b0651ffc/exceptiongroup-1.3.0-py3-none-any.whl.metadata\n",
      "    Using cached exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Collecting packaging>=23.2 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Obtaining dependency information for packaging>=23.2 from https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl.metadata\n",
      "    Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Collecting pathspec>=0.10.1 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Obtaining dependency information for pathspec>=0.10.1 from https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl.metadata\n",
      "    Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Collecting tomli>=1.2.2 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Obtaining dependency information for tomli>=1.2.2 from https://files.pythonhosted.org/packages/6e/c2/61d3e0f47e2b74ef40a68b9e6ad5984f6241a942f7cd3bbfbdbd03861ea9/tomli-2.2.1-py3-none-any.whl.metadata\n",
      "    Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Collecting typing-extensions>=4.6.0 (from exceptiongroup>=1.0->scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Obtaining dependency information for typing-extensions>=4.6.0 from https://files.pythonhosted.org/packages/b5/00/d631e67a838026495268c2f6884f3711a15a9a2a96cd244fdaea53b823fb/typing_extensions-4.14.1-py3-none-any.whl.metadata\n",
      "    Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached scikit_build_core-0.11.5-py3-none-any.whl (185 kB)\n",
      "  Using cached exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
      "  Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "  Installing collected packages: typing-extensions, tomli, pathspec, packaging, exceptiongroup, scikit-build-core\n",
      "  \u001b[?25l\n",
      "  \u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/6\u001b[0m [scikit-build-core]\n",
      "  \u001b[?25h\n",
      "  \u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "  torchaudio 2.2.0 requires torch==2.2.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "  \u001b[0mSuccessfully installed exceptiongroup-1.3.0 packaging-25.0 pathspec-0.12.1 scikit-build-core-0.11.5 tomli-2.2.1 typing-extensions-4.14.1\n",
      "  \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "  \u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Getting requirements to build wheel\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command pip subprocess to install backend dependencies\n",
      "  Using pip 25.1.1 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
      "  Collecting cmake>=3.21\n",
      "    Obtaining dependency information for cmake>=3.21 from https://files.pythonhosted.org/packages/60/cb/1e4d8baab7c946f809d6c59914428c10acaf39d9f4b52e1dffff834a9f0a/cmake-4.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "    Using cached cmake-4.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "  Using cached cmake-4.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
      "  Installing collected packages: cmake\n",
      "    Creating /tmp/pip-build-env-5kyy2iqc/normal/local/bin\n",
      "    changing mode of /tmp/pip-build-env-5kyy2iqc/normal/local/bin/ccmake to 755\n",
      "    changing mode of /tmp/pip-build-env-5kyy2iqc/normal/local/bin/cmake to 755\n",
      "    changing mode of /tmp/pip-build-env-5kyy2iqc/normal/local/bin/cpack to 755\n",
      "    changing mode of /tmp/pip-build-env-5kyy2iqc/normal/local/bin/ctest to 755\n",
      "  Successfully installed cmake-4.0.3\n",
      "  \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "  \u001b[0m\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Preparing metadata (pyproject.toml)\n",
      "  \u001b[32m*** \u001b[1mscikit-build-core 0.11.5\u001b[0m using \u001b[34mCMake 4.0.3\u001b[39m\u001b[0m \u001b[31m(metadata_wheel)\u001b[0m\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/b5/00/d631e67a838026495268c2f6884f3711a15a9a2a96cd244fdaea53b823fb/typing_extensions-4.14.1-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Link requires a different Python (3.10.12 not in: '>=3.11'): https://files.pythonhosted.org/packages/e7/b1/50f64d9a874841804dceb5d3e26d953114f97a35e331d2571320f1c14e51/numpy-2.3.0rc1.tar.gz (from https://pypi.org/simple/numpy/) (requires-python:>=3.11)\n",
      "  Link requires a different Python (3.10.12 not in: '>=3.11'): https://files.pythonhosted.org/packages/f3/db/8e12381333aea300890829a0a36bfa738cac95475d88982d538725143fd9/numpy-2.3.0.tar.gz (from https://pypi.org/simple/numpy/) (requires-python:>=3.11)\n",
      "  Link requires a different Python (3.10.12 not in: '>=3.11'): https://files.pythonhosted.org/packages/2e/19/d7c972dfe90a353dbd3efbbe1d14a5951de80c99c9dc1b93cd998d51dc0f/numpy-2.3.1.tar.gz (from https://pypi.org/simple/numpy/) (requires-python:>=3.11)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/b4/63/3de6a34ad7ad6646ac7d2f55ebc6ad439dbbf9c4370017c50cf403fb19b5/numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Obtaining dependency information for jinja2>=2.11.3 from https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl.metadata\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/22/35/137da042dfb4720b638d2937c38a9c2df83fe32d20e8c8f3185dbfef05f7/MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m234.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "  \u001b[32m*** \u001b[1mscikit-build-core 0.11.5\u001b[0m using \u001b[34mCMake 4.0.3\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "  \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  loading initial cache file /tmp/tmproxu65pw/build/CMakeInit.txt\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/x86_64-linux-gnu-gcc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/x86_64-linux-gnu-g++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  -- GGML_SYSTEM_ARCH: x86\n",
      "  -- Including CPU backend\n",
      "  -- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "  -- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "  -- Found OpenMP: TRUE (found version \"4.5\")\n",
      "  -- x86 detected\n",
      "  -- Adding CPU backend variant ggml-cpu: -march=native\n",
      "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.1.105\")\n",
      "  -- CUDA Toolkit found\n",
      "  -- Using CUDA architectures: native\n",
      "  -- The CUDA compiler identification is NVIDIA 12.1.105 with host compiler GNU 11.4.0\n",
      "  -- Detecting CUDA compiler ABI info\n",
      "  -- Detecting CUDA compiler ABI info - done\n",
      "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "  -- Detecting CUDA compile features\n",
      "  -- Detecting CUDA compile features - done\n",
      "  -- CUDA host compiler is GNU 11.4.0\n",
      "  -- Including CUDA backend\n",
      "  -- ggml version: 0.0.1\n",
      "  -- ggml commit:  79e0b68\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:108 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:108 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "    Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:109 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:109 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "    Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:162 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:162 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  -- Configuring done (7.4s)\n",
      "  -- Generating done (0.1s)\n",
      "  \u001b[33mCMake Warning:\n",
      "    Manually-specified variables were not used by the project:\n",
      "\n",
      "      CUDA_ARCHITECTURES\n",
      "\n",
      "  \u001b[0m\n",
      "  -- Build files have been written to: /tmp/tmproxu65pw/build\n",
      "  \u001b[32m***\u001b[0m \u001b[1mBuilding project with \u001b[34mNinja\u001b[39m...\u001b[0m\n",
      "  Change Dir: '/tmp/tmproxu65pw/build'\n",
      "\n",
      "  Run Build Command(s): /usr/local/bin/ninja -v -j 8\n",
      "  [1/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\\\"79e0b68\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-threading.cpp\n",
      "  [2/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\\\"79e0b68\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml.cpp\n",
      "  [3/177] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BUILD -DGGML_COMMIT=\\\"79e0b68\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-alloc.c\n",
      "  [4/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\\\"79e0b68\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-backend.cpp\n",
      "  [5/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp\n",
      "  [6/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/hbm.cpp\n",
      "  [7/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\\\"79e0b68\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-opt.cpp\n",
      "  [8/177] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c\n",
      "  [9/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-backend-reg.cpp\n",
      "  [10/177] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BUILD -DGGML_COMMIT=\\\"79e0b68\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml.c\n",
      "  [11/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/traits.cpp\n",
      "  [12/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp\n",
      "  [13/177] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/quants.c\n",
      "  [14/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp\n",
      "  [15/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/vec.cpp\n",
      "  [16/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\\\"79e0b68\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/gguf.cpp\n",
      "  [17/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp\n",
      "  [18/177] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/quants.c\n",
      "  [19/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp\n",
      "  [20/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp\n",
      "  [21/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/repack.cpp\n",
      "  [22/177] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BUILD -DGGML_COMMIT=\\\"79e0b68\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-quants.c\n",
      "  [23/177] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml-base.so -o bin/libggml-base.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  -lm && :\n",
      "  [24/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp\n",
      "  [25/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/acc.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\n",
      "  [26/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/arange.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\n",
      "  [27/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cpu/ops.cpp\n",
      "  [28/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/argsort.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\n",
      "  [29/177] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml-cpu.so -o bin/libggml-cpu.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  bin/libggml-base.so  /usr/lib/gcc/x86_64-linux-gnu/11/libgomp.so  /usr/lib/x86_64-linux-gnu/libpthread.a && :\n",
      "  [30/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/argmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\n",
      "  [31/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/clamp.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\n",
      "  [32/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/concat.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\n",
      "  [33/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\n",
      "  [34/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/binbcast.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\n",
      "  [35/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/count-equal.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\n",
      "  [36/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/conv2d-transpose.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\n",
      "  [37/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/conv2d-dw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\n",
      "  [38/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\n",
      "  [39/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\n",
      "  [40/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/diagmask.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\n",
      "  [41/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/cpy.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\n",
      "  [42/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/fattn.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\n",
      "  [43/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o\n",
      "  [44/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o\n",
      "  [45/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\n",
      "  [46/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/gla.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\n",
      "  [47/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/mean.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\n",
      "  [48/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/im2col.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\n",
      "  [49/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\n",
      "  [50/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/mmq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\n",
      "  [51/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\n",
      "  [52/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\n",
      "  /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu: In function â€˜bool ggml_backend_cuda_device_supports_op(ggml_backend_dev_t, const ggml_tensor*)â€™:\n",
      "  /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:3229:133: note: â€˜#pragma message: TODO: implement Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, IQ4_NL support (https://github.com/ggml-org/llama.cpp/pull/14661)â€™\n",
      "   3229 | #pragma message(\"TODO: implement Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, IQ4_NL support (https://github.com/ggml-org/llama.cpp/pull/14661)\")\n",
      "        |                                                                                                                                     ^\n",
      "  [53/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/out-prod.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\n",
      "  [54/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/pad.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\n",
      "  [55/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/norm.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\n",
      "  [56/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/pool2d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\n",
      "  [57/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/quantize.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\n",
      "  [58/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/scale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\n",
      "  [59/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\n",
      "  [60/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/rope.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\n",
      "  [61/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/ssm-conv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\n",
      "  [62/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/softmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\n",
      "  [63/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/ssm-scan.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\n",
      "  [64/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/sumrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\n",
      "  [65/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/tsembd.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\n",
      "  [66/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/upscale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\n",
      "  [67/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/unary.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\n",
      "  [68/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/sum.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\n",
      "  [69/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/wkv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\n",
      "  [70/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\n",
      "  [71/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\n",
      "  [72/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\n",
      "  [73/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/mmv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o\n",
      "  [74/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\n",
      "  [75/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\n",
      "  [76/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/mmvq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\n",
      "  [77/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\n",
      "  [78/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\n",
      "  [79/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\n",
      "  [80/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\n",
      "  [81/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\n",
      "  [82/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\n",
      "  [83/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\n",
      "  [84/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\n",
      "  [85/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\n",
      "  [86/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\n",
      "  [87/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\n",
      "  [88/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\n",
      "  [89/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\n",
      "  [90/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\n",
      "  [91/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\n",
      "  [92/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\n",
      "  [93/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\n",
      "  [94/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\n",
      "  [95/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\n",
      "  [96/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\n",
      "  [97/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\n",
      "  [98/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\n",
      "  [99/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\n",
      "  [100/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\n",
      "  [101/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\n",
      "  [102/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\n",
      "  [103/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\n",
      "  [104/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\n",
      "  [105/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o\n",
      "  [106/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o\n",
      "  [107/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o\n",
      "  [108/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\n",
      "  [109/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o\n",
      "  [110/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o\n",
      "  [111/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o\n",
      "  [112/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o\n",
      "  [113/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama.cpp\n",
      "  [114/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-adapter.cpp\n",
      "  [115/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o\n",
      "  [116/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-arch.cpp\n",
      "  [117/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o\n",
      "  [118/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-cparams.cpp\n",
      "  [119/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-chat.cpp\n",
      "  [120/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-batch.cpp\n",
      "  [121/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-hparams.cpp\n",
      "  [122/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-impl.cpp\n",
      "  [123/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-io.cpp\n",
      "  [124/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o\n",
      "  [125/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-graph.cpp\n",
      "  [126/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-memory.cpp\n",
      "  [127/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-context.cpp\n",
      "  [128/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-kv-cache-unified-iswa.cpp\n",
      "  [129/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\n",
      "  [130/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-memory-hybrid.cpp\n",
      "  [131/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-mmap.cpp\n",
      "  [132/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-memory-recurrent.cpp\n",
      "  [133/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-kv-cache-unified.cpp\n",
      "  [134/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-model-saver.cpp\n",
      "  [135/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-model-loader.cpp\n",
      "  [136/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-grammar.cpp\n",
      "  [137/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\n",
      "  [138/177] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/tmproxu65pw/build/vendor/llama.cpp/common/build-info.cpp\n",
      "  [139/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/unicode-data.cpp\n",
      "  [140/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-vocab.cpp\n",
      "  [141/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-quant.cpp\n",
      "  [142/177] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o.d -x cu -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\n",
      "  [143/177] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml-cuda.so -o bin/libggml-cuda.so vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o  -Wl,-rpath,\"\\$ORIGIN\"  bin/libggml-base.so  /usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.1/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.1/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.1/targets/x86_64-linux/lib/libculibos.a  -ldl  /usr/lib/x86_64-linux-gnu/librt.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\" && :\n",
      "  [144/177] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml.so -o bin/libggml.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  -ldl  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  -ldl && :\n",
      "  [145/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-sampling.cpp\n",
      "  [146/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/console.cpp\n",
      "  [147/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/chat-parser.cpp\n",
      "  [148/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/llguidance.cpp\n",
      "  [149/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/log.cpp\n",
      "  [150/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/common.cpp\n",
      "  [151/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/unicode.cpp\n",
      "  [152/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/ngram-cache.cpp\n",
      "  [153/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/speculative.cpp\n",
      "  [154/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/sampling.cpp\n",
      "  [155/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/json-partial.cpp\n",
      "  [156/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/mtmd-audio.cpp\n",
      "  [157/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/mtmd.cpp\n",
      "  [158/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/llama-model.cpp\n",
      "  [159/177] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o bin/libllama.so vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
      "  [160/177] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp\n",
      "  [161/177] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-llava-cli   && :\n",
      "  [162/177] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp\n",
      "  [163/177] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-gemma3-cli   && :\n",
      "  [164/177] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp\n",
      "  [165/177] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-minicpmv-cli   && :\n",
      "  [166/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/regex-partial.cpp\n",
      "  [167/177] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp\n",
      "  [168/177] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-qwen2vl-cli   && :\n",
      "  [169/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/include -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/mtmd-cli.cpp\n",
      "  [170/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/clip.cpp\n",
      "  [171/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/json-schema-to-grammar.cpp\n",
      "  [172/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/arg.cpp\n",
      "  [173/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/tools/mtmd/mtmd-helper.cpp\n",
      "  [174/177] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libmtmd.so -o vendor/llama.cpp/tools/mtmd/libmtmd.so vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
      "  [175/177] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/. -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/src/../include -I/tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o -c /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/vendor/llama.cpp/common/chat.cpp\n",
      "  [176/177] : && /tmp/pip-build-env-5kyy2iqc/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/x86_64-linux-gnu-ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o && /usr/bin/x86_64-linux-gnu-ranlib vendor/llama.cpp/common/libcommon.a && :\n",
      "  [177/177] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-mtmd-cli  -Wl,-rpath,/tmp/tmproxu65pw/build/vendor/llama.cpp/tools/mtmd:/tmp/tmproxu65pw/build/bin:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/tools/mtmd/libmtmd.so  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
      "\n",
      "  \u001b[32m***\u001b[0m \u001b[1mInstalling project into wheel...\u001b[0m\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/libggml-cpu.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/libggml-cuda.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/libggml.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-cpu.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-alloc.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-backend.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-blas.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-cann.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-cpp.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-cuda.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-opt.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-metal.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-rpc.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-sycl.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/ggml-vulkan.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/gguf.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/libggml-base.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/cmake/ggml/ggml-config.cmake\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/cmake/ggml/ggml-version.cmake\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/libllama.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/llama.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/llama-cpp.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/cmake/llama/llama-config.cmake\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/cmake/llama/llama-version.cmake\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/bin/convert_hf_to_gguf.py\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/pkgconfig/llama.pc\n",
      "  -- Installing: /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/llama_cpp/lib/libllama.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/llama_cpp/lib/libllama.so\n",
      "  -- Installing: /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/llama_cpp/lib/libggml.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/llama_cpp/lib/libggml.so\n",
      "  -- Installing: /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/llama_cpp/lib/libggml-base.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/llama_cpp/lib/libggml-base.so\n",
      "  -- Installing: /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/llama_cpp/lib/libggml-cpu.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/llama_cpp/lib/libggml-cpu.so\n",
      "  -- Installing: /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/llama_cpp/lib/libggml-cuda.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/llama_cpp/lib/libggml-cuda.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/lib/libmtmd.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/mtmd.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/include/mtmd-helper.h\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/bin/llama-mtmd-cli\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmproxu65pw/wheel/platlib/bin/llama-mtmd-cli\" to \"\"\n",
      "  -- Installing: /tmp/pip-install-aafr5zx5/llama-cpp-python_2957b543f88148498cfd38ef1c030998/llama_cpp/lib/libmtmd.so\n",
      "  -- Installing: /tmp/tmproxu65pw/wheel/platlib/llama_cpp/lib/libmtmd.so\n",
      "  \u001b[32m***\u001b[0m \u001b[1mMaking wheel...\u001b[0m\n",
      "  \u001b[32m***\u001b[0m \u001b[1mCreated\u001b[22m llama_cpp_python-0.3.14-cp310-cp310-linux_x86_64.whl\u001b[0m\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.14-cp310-cp310-linux_x86_64.whl size=45484983 sha256=4c0e9e40fe424e9f401a1cffc6278c282fa42cfd75fd79ba72ef80216030d7d4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tpz1q5it/wheels/b8/76/4f/9c25e015350172f4e6f3bbe549ffd87005a5764aa21848b5b3\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.1\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.1:\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.14.1.dist-info/\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.1/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: numpyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6\u001b[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Removing file or directory /usr/local/bin/f2pyâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Removing file or directory /usr/local/bin/numpy-configâ”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-2.2.6.dist-info/\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/py]\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/ [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  changing mode of /usr/local/bin/f2py to 755â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  changing mode of /usr/local/bin/numpy-config to 755â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafeâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 2.1.5â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling MarkupSafe-2.1.5:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/markupsafe/py]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-2.1.5â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: diskcacheâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: diskcache 5.6.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling diskcache-5.6.3:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/mpy]\n",
      "\u001b[2K      Successfully uninstalled diskcache-5.6.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: jinja20m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/6\u001b[0m [diskcache]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/6\u001b[0m [diskcache]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.3:90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/6\u001b[0m [diskcache]\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/Jinja2-3.1.3.dist-info/\n",
      "\u001b[2K      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2/[diskcache]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.3â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/6\u001b[0m [diskcache]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/6\u001b[0m [llama-cpp-python][llama-cpp-python]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.0 requires torch==2.2.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.14 numpy-2.2.6 typing-extensions-4.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mâœ… llama-cpp-python with RTX 4090 optimization installed!\n"
     ]
    }
   ],
   "source": [
    "# Install llama-cpp-python with CUDA support for RTX 4090 (GPU-accelerated compilation)\n",
    "print(\"ðŸ“¦ Installing llama-cpp-python with CUDA support...\")\n",
    "print(\"Using RTX 4090 for accelerated compilation...\")\n",
    "\n",
    "# Use GPU-accelerated compilation with multiple parallel jobs\n",
    "import os\n",
    "os.environ['CMAKE_ARGS'] = '-DGGML_CUDA=on -DCUDA_ARCHITECTURES=89'  # RTX 4090 is compute capability 8.9\n",
    "os.environ['CUDACXX'] = '/usr/local/cuda/bin/nvcc'  # Ensure CUDA compiler is found\n",
    "os.environ['CMAKE_BUILD_PARALLEL_LEVEL'] = '8'  # Use 8 parallel compilation jobs\n",
    "\n",
    "!pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir --verbose\n",
    "\n",
    "print(\"âœ… llama-cpp-python with RTX 4090 optimization installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing complete system with llama-cpp-python...\n",
      "âœ… All modules imported successfully\n",
      "âœ… CUDA available: NVIDIA GeForce RTX 4090\n",
      "   CUDA version: 12.6\n",
      "   GPU memory: 23.6 GB\n",
      "âœ… Configuration loaded\n",
      "ðŸ¤– Testing LLM service...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM model loaded and working!\n",
      "ðŸ¤– Test response: Hello! I'm an AI here to assist you. While I don't work in\n",
      "âœ… System ready for document processing!\n",
      "\n",
      "ðŸŽ‰ Complete setup successful!\n",
      "\n",
      "Next steps:\n",
      "1. Build index: python run_search.py --build /path/to/your/documents\n",
      "2. Start web UI: python run_search.py --web\n",
      "3. Or search directly: python run_search.py --search 'your question'\n"
     ]
    }
   ],
   "source": [
    "# Test the complete system with llama-cpp-python installed\n",
    "try:\n",
    "    print(\"ðŸ§ª Testing complete system with llama-cpp-python...\")\n",
    "    \n",
    "    # Test imports\n",
    "    from config import Config\n",
    "    from document_processor import DocumentProcessor\n",
    "    from embedding_service import EmbeddingService\n",
    "    from vector_database import VectorDatabase\n",
    "    from llm_service_cpp import LLMServiceCPP\n",
    "    \n",
    "    print(\"âœ… All modules imported successfully\")\n",
    "    \n",
    "    # Test CUDA availability\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"âš ï¸  CUDA not available - will use CPU mode\")\n",
    "    \n",
    "    # Test model loading\n",
    "    config = Config()\n",
    "    config.create_directories()\n",
    "    \n",
    "    print(\"âœ… Configuration loaded\")\n",
    "    \n",
    "    # Test LLM service\n",
    "    print(\"ðŸ¤– Testing LLM service...\")\n",
    "    llm_service = LLMServiceCPP(config)\n",
    "    \n",
    "    if llm_service.check_model_availability():\n",
    "        print(\"âœ… LLM model loaded and working!\")\n",
    "        \n",
    "        # Test a simple generation\n",
    "        test_response = llm_service.llm(\n",
    "            \"<|system|>You are a helpful assistant.<|user|>Say 'Hello, I am working!'<|assistant|>\",\n",
    "            max_tokens=20,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        print(f\"ðŸ¤– Test response: {test_response['choices'][0]['text'].strip()}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ LLM model not responding correctly\")\n",
    "    \n",
    "    print(\"âœ… System ready for document processing!\")\n",
    "    print(\"\\nðŸŽ‰ Complete setup successful!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Build index: python run_search.py --build /path/to/your/documents\")\n",
    "    print(\"2. Start web UI: python run_search.py --web\")\n",
    "    print(\"3. Or search directly: python run_search.py --search 'your question'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ System test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"- Make sure all dependencies are installed\")\n",
    "    print(\"- Check that the model file exists at models/llama-model.gguf\")\n",
    "    print(\"- Verify CUDA installation if using GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
