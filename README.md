# ğŸ” Intelligent Document Search System

A high-performance document search system that combines **Docling** for document analysis, **BGE-3** embeddings for semantic search, **FAISS** vector database for fast retrieval, and **llama.cpp** for local LLM processing.

## ğŸš€ Features

- **Smart Document Processing**: Uses Docling to extract and analyze content from PDFs, DOCX, TXT, HTML, and Markdown files
- **High-Accuracy Embeddings**: BGE-large-en-v1.5 model for superior semantic understanding
- **GPU-Accelerated**: Optimized for RTX 4090 with CUDA acceleration and mixed precision
- **Fast Vector Search**: FAISS with GPU acceleration for instant similarity search
- **Local LLM**: llama.cpp with GGUF models for privacy-preserving AI responses
- **Incremental Updates**: Build once, search many times - no need to rebuild from scratch
- **Source Attribution**: Click-through links to original documents
- **Web Interface**: Clean Streamlit UI for easy interaction
- **Command Line**: Powerful CLI for automation and scripting

## ğŸ“‹ Quick Start

### 1. Setup
```bash
python run_search.py --setup
```

### 2. Download a Model
Choose one of these models:
```bash
# Option 1: Phi-3 Mini (3.8B, recommended)
wget -P models/ https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
mv models/Phi-3-mini-4k-instruct-q4.gguf models/llama-model.gguf

# Option 2: Llama 3.1 8B (larger, more capable)
wget -P models/ https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
mv models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf models/llama-model.gguf
```

### 3. Build Index
```bash
python run_search.py --build /path/to/your/documents
```

### 4. Start Searching
```bash
# Web interface
python run_search.py --web

# Command line search
python run_search.py --search "your question here"

# Interactive mode
python run_search.py --interactive
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Documents     â”‚â”€â”€â”€â–¶â”‚     Docling      â”‚â”€â”€â”€â–¶â”‚    Chunking     â”‚
â”‚  (PDF/DOCX/etc) â”‚    â”‚   Processing     â”‚    â”‚   & Metadata   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚â”€â”€â”€â–¶â”‚  BGE Embeddings  â”‚â”€â”€â”€â–¶â”‚  Vector Search  â”‚
â”‚                 â”‚    â”‚  (GPU Optimized) â”‚    â”‚   (FAISS GPU)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Answer   â”‚â—€â”€â”€â”€â”‚   llama.cpp      â”‚â—€â”€â”€â”€â”‚   Retrieved     â”‚
â”‚  with Sources   â”‚    â”‚  Local LLM       â”‚    â”‚   Documents     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration

Key settings in `config.py`:

```python
# GPU Optimization (RTX 4090)
CUDA_BATCH_SIZE = 128
USE_MIXED_PRECISION = True
N_GPU_LAYERS = -1  # Use all GPU layers

# Embedding Model
EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5"

# Document Processing
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# Search Parameters
TOP_K_RESULTS = 5
SIMILARITY_THRESHOLD = 0.7
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ config.py                 # Configuration settings
â”œâ”€â”€ document_processor.py     # Docling-based document processing
â”œâ”€â”€ embedding_service.py      # BGE embedding generation
â”œâ”€â”€ vector_database.py        # FAISS vector storage
â”œâ”€â”€ llm_service_cpp.py        # llama.cpp integration
â”œâ”€â”€ search_interface.py       # Streamlit web interface
â”œâ”€â”€ main_pipeline.py          # Core pipeline logic
â”œâ”€â”€ run_search.py             # Easy-to-use runner script
â””â”€â”€ requirements.txt          # Python dependencies
```

## ğŸ¯ Usage Examples

### Command Line

```bash
# Build index from multiple directories
python run_search.py --build /docs/research /docs/papers /docs/reports

# Search with custom parameters
python main_pipeline.py --search "machine learning algorithms" --top-k 10 --threshold 0.8

# Force rebuild index
python run_search.py --build /docs --force-rebuild

# Get index statistics
python main_pipeline.py --stats
```

### Web Interface

1. Start the web interface: `python run_search.py --web`
2. Upload documents or specify directories
3. Search with natural language queries
4. Get AI-generated answers with source attribution
5. Click on sources to view original documents

### Python API

```python
from main_pipeline import DocumentSearchPipeline

# Initialize pipeline
pipeline = DocumentSearchPipeline()

# Build index (one-time)
pipeline.build_index(["/path/to/docs"])

# Search
results = pipeline.search("your question", top_k=5, use_llm=True)
print(results['llm_response']['answer'])
```

## ğŸ” Search Features

- **Semantic Search**: Understanding intent beyond keywords
- **Source Attribution**: Every answer links back to source documents
- **Relevance Scoring**: Confidence scores for each result
- **Context Highlighting**: See exactly which parts were used
- **Multi-format Support**: PDF, DOCX, TXT, HTML, Markdown
- **Incremental Updates**: Add new documents without full rebuild

## âš¡ Performance Optimizations

### GPU Acceleration (RTX 4090)
- CUDA-accelerated embedding generation
- Mixed precision inference (FP16)
- GPU-accelerated FAISS indexing
- Optimized batch sizes for 24GB VRAM

### Memory Efficiency
- Memory-mapped model loading
- Batch processing for large document sets
- Efficient chunking strategies
- Lazy loading of embeddings

### Speed Optimizations
- Pre-computed embeddings stored on disk
- FAISS IVF indexing for fast similarity search
- llama.cpp for efficient local inference
- Smart caching of frequent queries

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Model not found**: Ensure you've downloaded a GGUF model to `models/llama-model.gguf`

**CUDA errors**: Verify NVIDIA drivers and CUDA toolkit installation

**Memory issues**: Reduce batch size in config for smaller GPUs

**Slow performance**: Check GPU utilization and enable mixed precision

### Debug Commands

```bash
# Check model availability
python -c "from llm_service_cpp import LLMServiceCPP; from config import Config; svc = LLMServiceCPP(Config()); print(svc.check_model_availability())"

# Verify GPU setup
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')"

# Test embeddings
python -c "from embedding_service import EmbeddingService; from config import Config; svc = EmbeddingService(Config()); print('Embeddings working')"
```

## ğŸ“Š Supported Formats

| Format | Extension | Processing Method |
|--------|-----------|-------------------|
| PDF | `.pdf` | Docling with layout analysis |
| Word | `.docx` | Docling structure extraction |
| Text | `.txt` | Direct text processing |
| HTML | `.html` | Content extraction |
| Markdown | `.md` | Structured parsing |

## ğŸ”’ Privacy & Security

- **Local Processing**: All data stays on your machine
- **No External APIs**: Complete offline operation
- **Source Control**: Track exactly which documents are used
- **Secure Storage**: Encrypted vector database options available

## ğŸ“ˆ Scaling

The system scales efficiently:
- **Documents**: Tested with 100K+ documents
- **Search Speed**: Sub-second response times
- **Memory Usage**: Configurable based on available hardware
- **Storage**: Compressed vector indices for large datasets

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“ License

MIT License - see LICENSE file for details.

---

**Happy Searching! ğŸ”âœ¨**